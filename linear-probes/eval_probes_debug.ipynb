{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append('.')\n",
    "import torch as t\n",
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pandas import DataFrame, json_normalize\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "from datasets import TruthfulQADataset, DishonestQADataset, AmongUsDataset, RolePlayingDataset, RepEngDataset\n",
    "from evaluate_utils import evaluate_probe_on_activation_dataset\n",
    "from configs import config_phi4, config_gpt2, config_llama3\n",
    "from plots import plot_behavior_distribution, plot_roc_curves, add_roc_curves, print_metrics, plot_roc_curve_eval\n",
    "import probes\n",
    "from pprint import pprint as pp\n",
    "\n",
    "config = config_phi4\n",
    "model, tokenizer, device = None, None, 'cpu'\n",
    "\n",
    "from datasets import (\n",
    "    TruthfulQADataset,\n",
    "    DishonestQADataset, \n",
    "    AmongUsDataset,\n",
    "    RepEngDataset,\n",
    ")\n",
    "from probes import LinearProbe\n",
    "\n",
    "datasets: List[str] = [\n",
    "    \"TruthfulQADataset\",\n",
    "    \"DishonestQADataset\",\n",
    "    \"AmongUsDataset\",\n",
    "    \"RepEngDataset\",\n",
    "    # \"RolePlayingDataset\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_probe(\n",
    "    dataset_name: str,\n",
    "    probe: LinearProbe,\n",
    "    config: Dict[str, Any], \n",
    "    model=None, \n",
    "    tokenizer= None,\n",
    "    eval_TruthfulQA: bool = True,\n",
    "    eval_DishonestQA: bool = True,\n",
    "    eval_RepEng: bool = True,\n",
    "    eval_AmongUs: bool = True,\n",
    "    ) -> None:\n",
    "\n",
    "    rocs = {}\n",
    "    # make a directory to save the results for this dataset inside results/dataset_name\n",
    "    os.makedirs(f\"results/{dataset_name}_{config['short_name']}\", exist_ok=True)\n",
    "    # remove the old results\n",
    "    for file in os.listdir(f\"results/{dataset_name}_{config['short_name']}\"):\n",
    "        if file.endswith(\".json\") or file.endswith(\".pdf\"):\n",
    "            os.remove(os.path.join(f\"results/{dataset_name}_{config['short_name']}\", file))\n",
    "    \n",
    "    # evaluate on TQA\n",
    "    if eval_TruthfulQA:\n",
    "        dataset = TruthfulQADataset(config, model=model, tokenizer=tokenizer, device=device, test_split=0.2)\n",
    "        test_acts_chunk = dataset.get_test_acts()\n",
    "        av_probe_outputs, accuracy = evaluate_probe_on_activation_dataset(\n",
    "            chunk_data=test_acts_chunk,\n",
    "            probe=probe,\n",
    "            device=device,\n",
    "            num_tokens=None,\n",
    "        )\n",
    "        labels = t.tensor([batch[1] for batch in test_acts_chunk]).numpy()\n",
    "        fpr, tpr, _ = roc_curve(labels, av_probe_outputs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        rocs[\"TQA\"] = {\"fpr\": fpr.tolist(), \"tpr\": tpr.tolist(), \"auc\": roc_auc}\n",
    "\n",
    "    # evaluate on DQA\n",
    "    if eval_DishonestQA:\n",
    "        dataset = DishonestQADataset(config, model=model, tokenizer=tokenizer, device=device, test_split=0.2)\n",
    "        test_acts_chunk = dataset.get_test_acts()\n",
    "        av_probe_outputs, accuracy = evaluate_probe_on_activation_dataset(\n",
    "            chunk_data=test_acts_chunk,\n",
    "            probe=probe,\n",
    "            device=device,\n",
    "            num_tokens=None,\n",
    "        )\n",
    "        labels = t.tensor([batch[1] for batch in test_acts_chunk]).numpy()\n",
    "        fpr, tpr, _ = roc_curve(labels, av_probe_outputs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        rocs[\"DQA\"] = {\"fpr\": fpr.tolist(), \"tpr\": tpr.tolist(), \"auc\": roc_auc}\n",
    "\n",
    "    # evaluate on RepEng\n",
    "    if eval_RepEng:\n",
    "        dataset = RepEngDataset(config, model=model, tokenizer=tokenizer, device=device, test_split=0.2)\n",
    "        test_acts_chunk = dataset.get_test_acts()\n",
    "        av_probe_outputs, accuracy = evaluate_probe_on_activation_dataset(\n",
    "            chunk_data=test_acts_chunk,\n",
    "            probe=probe,\n",
    "            device=device,\n",
    "            num_tokens=None,\n",
    "        )\n",
    "        labels = t.tensor([batch[1] for batch in test_acts_chunk]).numpy()\n",
    "        fpr, tpr, _ = roc_curve(labels, av_probe_outputs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        rocs[\"RepEng\"] = {\"fpr\": fpr.tolist(), \"tpr\": tpr.tolist(), \"auc\": roc_auc}\n",
    "\n",
    "    # evaluate on AmongUs\n",
    "    if eval_AmongUs:\n",
    "        dataset = AmongUsDataset(config, model=model, tokenizer=tokenizer, device=device, expt_name=config['expt_name'], test_split=1)\n",
    "        all_probe_outputs = []\n",
    "        chunk_size: int = 500\n",
    "        list_of_chunks_to_eval = [1, 2]\n",
    "        row_indices = []\n",
    "\n",
    "        for chunk_idx in tqdm(list_of_chunks_to_eval):\n",
    "            test_acts_chunk = dataset.get_test_acts(chunk_idx)\n",
    "            \n",
    "            # Store the row indices for this chunk\n",
    "            start_idx = chunk_idx * chunk_size\n",
    "            end_idx = start_idx + len(test_acts_chunk)\n",
    "            row_indices.extend(range(start_idx, end_idx))\n",
    "            \n",
    "            chunk_probe_outputs, _ = evaluate_probe_on_activation_dataset(\n",
    "                chunk_data=test_acts_chunk,\n",
    "                probe=probe,\n",
    "                device=device,\n",
    "                num_tokens=None,\n",
    "                verbose=False,\n",
    "            )\n",
    "            all_probe_outputs.extend(chunk_probe_outputs)\n",
    "\n",
    "        av_probe_outputs = all_probe_outputs\n",
    "\n",
    "        json_outputs = []\n",
    "        eval_rows_num = len(av_probe_outputs)\n",
    "\n",
    "        for i in range(eval_rows_num):\n",
    "            actual_row_idx = row_indices[i]\n",
    "            row = dataset.agent_logs_df.iloc[actual_row_idx]\n",
    "            probe_output = av_probe_outputs[i]  # Use the pre-calculated average probe outputs\n",
    "            \n",
    "            if (eval_rows_num > 10 and i % (eval_rows_num // 10) == 0) or (eval_rows_num <= 10):\n",
    "                print(f\"Evaluated {i}/{eval_rows_num} rows, predicted {probe_output}\")\n",
    "\n",
    "            json_output = {\n",
    "                \"game_index\": int(row[\"game_index\"].split(\" \")[1]) if isinstance(row[\"game_index\"], str) else int(row[\"game_index\"]),\n",
    "                \"step\": int(row[\"step\"]),\n",
    "                \"player_name\": row[\"player.name\"],\n",
    "                \"probe_output\": probe_output,\n",
    "                \"timestamp\": row[\"timestamp\"],\n",
    "                \"player_role\": row[\"player.personality\"],\n",
    "            }\n",
    "            json_outputs.append(json_output)\n",
    "\n",
    "        probe_output_df = pd.DataFrame(json_outputs)\n",
    "        \n",
    "        EXPT_NAMES: List[str] = [config[\"expt_name\"],]\n",
    "        LOGS_PATH: str = \"../evaluations/results/\"\n",
    "        RAW_PATH: str = \"../expt-logs/\"\n",
    "        DESCRIPTIONS: List[str] = [\"Crew: Phi, Imp: Phi\",]\n",
    "\n",
    "        summary_logs_paths: List[str] = [os.path.join(LOGS_PATH, f\"{expt_name}_all_skill_scores.json\") for expt_name in EXPT_NAMES]\n",
    "        from utils import read_jsonl_as_json\n",
    "\n",
    "        summary_dfs: List[pd.DataFrame] = []\n",
    "\n",
    "        for summary_logs_path in summary_logs_paths:\n",
    "            # read json line by line\n",
    "            summary_logs: List[Dict[str, Any]] = read_jsonl_as_json(summary_logs_path)\n",
    "            summary_df: pd.DataFrame = json_normalize(summary_logs)\n",
    "            # sort by game_index and then step\n",
    "            summary_df = summary_df.sort_values(by=[\"game_index\", \"step\"])\n",
    "            summary_dfs.append(summary_df)\n",
    "            print(f\"Loaded {len(summary_df)} logs from {summary_logs_path}\")\n",
    "        \n",
    "        summary_df['game_index'] = summary_df['game_index'].str.extract('(\\d+)').astype(int) if summary_df['game_index'].dtype == 'object' else summary_df['game_index'].astype(int)\n",
    "\n",
    "        probe_df = probe_output_df[['game_index', 'step', 'player_name', 'probe_output', 'timestamp']].copy()\n",
    "        summary_df = summary_df[['timestamp', 'game_index', 'step', 'player_identity', 'awareness', 'lying', 'deception', 'planning', 'player_name', 'action']].copy()\n",
    "        probe_eval = pd.merge(\n",
    "            probe_df,\n",
    "            summary_df[['game_index', 'step', 'player_name', 'player_identity', 'awareness', 'lying', 'deception', 'planning', 'action', 'timestamp']],\n",
    "            on=['timestamp'],\n",
    "            how='inner'\n",
    "        )\n",
    "\n",
    "        probe_eval['lying'] = 10 - probe_eval['lying'].astype(int)\n",
    "        probe_eval['deception'] = 10 - probe_eval['deception'].astype(int)\n",
    "\n",
    "        # Convert behavior scores to numeric and binary ground truth (>5 is lying)\n",
    "        behaviors = ['lying', 'deception', 'planning', 'awareness']\n",
    "        for col in behaviors:\n",
    "            probe_eval[col] = probe_eval[col].astype(int)\n",
    "            probe_eval[f'{col}_truth'] = (probe_eval[col] > 5).astype(int)\n",
    "        \n",
    "        # Calculate ROC for lying and deception\n",
    "        fpr_lying, tpr_lying, _ = roc_curve(probe_eval['lying_truth'], probe_eval['probe_output'])\n",
    "        roc_auc_lying = auc(fpr_lying, tpr_lying)\n",
    "        \n",
    "        fpr_deception, tpr_deception, _ = roc_curve(probe_eval['deception_truth'], probe_eval['probe_output'])\n",
    "        roc_auc_deception = auc(fpr_deception, tpr_deception)\n",
    "        \n",
    "        rocs[\"AmongUs\"] = [\n",
    "            {\"name\": \"lying\", \"fpr\": fpr_lying.tolist(), \"tpr\": tpr_lying.tolist(), \"auc\": roc_auc_lying},\n",
    "            {\"name\": \"deception\", \"fpr\": fpr_deception.tolist(), \"tpr\": tpr_deception.tolist(), \"auc\": roc_auc_deception}\n",
    "        ]\n",
    "\n",
    "    return rocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer 21/39...\n",
      "Loaded probe trained on DishonestQADataset for layer 21.\n"
     ]
    }
   ],
   "source": [
    "layer = 21\n",
    "print(f\"Processing layer {layer}/{config['num_layers']-1}...\")\n",
    "probe_dataset_name = \"DishonestQADataset\"\n",
    "probe = LinearProbe(config[\"activation_size\"])\n",
    "\n",
    "checkpoint_path = f'./checkpoints/{probe_dataset_name}_probe_{config[\"short_name\"]}_layer{layer}.pkl'\n",
    "with open(checkpoint_path, 'rb') as f:\n",
    "    probe.model = pickle.load(f).model\n",
    "    print(f'Loaded probe trained on {probe_dataset_name} for layer {layer}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 0/316\tProbe outputs: [0.963, 0.7256, 0.95, 0.9657, 0.946, 0.7117, 0.9868, 0.7241, 0.9935, 0.5662, 0.9885, 0.9876, 0.9948, 0.762, 0.991, 0.9999, 1.0, 0.9986, 1.0, 0.9995, 1.0, 0.8975, 0.0792, 0.9637, 0.7508, 0.9927]\n",
      "Evaluating 32/316\tProbe outputs: [0.963, 0.7256, 0.95, 0.9979, 0.4958, 0.979, 0.8657, 0.9526, 0.9925, 0.7647, 0.9359, 0.6831, 0.3607, 0.8352, 0.9951, 0.9984, 0.9893, 0.8589, 0.5486, 0.9954, 0.8165, 0.8998, 0.992, 0.9927, 1.0, 1.0, 1.0, 0.9997, 0.9999, 0.9998, 1.0, 0.9796, 0.9998, 0.9789, 0.9995, 0.9999, 0.9996, 0.9968, 0.9999]\n",
      "Evaluating 64/316\tProbe outputs: [0.963, 0.7256, 0.95, 0.9657, 0.9979, 0.9371, 0.6444, 0.9235, 0.9927, 0.9963, 0.9999, 0.9848, 0.9712, 0.6681, 0.9054, 0.9999, 1.0, 0.9999, 0.9998, 0.9999, 1.0, 0.9993, 0.9997, 0.999, 0.9998, 1.0, 0.9927, 1.0]\n",
      "Evaluating 96/316\tProbe outputs: [0.963, 0.7256, 0.95, 0.9657, 0.9871, 0.925, 0.9995, 0.8749, 0.341, 0.9589, 0.9551, 0.936, 0.9945, 0.998, 0.8987, 0.9949, 0.9993, 1.0, 0.9988, 0.9984, 0.8155, 0.9944, 0.9996, 0.9928, 0.9961, 0.9999, 0.9918, 0.9998, 1.0, 0.9811, 0.9676, 0.9997]\n",
      "Evaluating 128/316\tProbe outputs: [0.963, 0.7256, 0.95, 0.9925, 0.9922, 0.9688, 0.8771, 0.968, 0.9527, 0.9275, 0.9804, 0.9959, 0.9979, 0.9999, 0.9987, 0.9976, 0.8722, 0.9999, 0.996, 1.0, 1.0, 0.9999, 1.0, 0.9991, 0.9983, 0.9989, 0.9876, 0.9998, 1.0]\n",
      "Evaluating 160/316\tProbe outputs: [0.963, 0.7256, 0.95, 0.9657, 0.9451, 0.99, 0.9283, 0.9973, 0.9764, 0.9972, 0.8455, 0.9999, 0.9996, 0.999, 0.7559, 0.9993, 0.9998, 1.0, 0.9995, 0.9945, 1.0, 1.0, 0.9999, 0.9998, 0.9988, 1.0, 1.0, 0.9968, 0.9942, 0.9951]\n",
      "Evaluating 192/316\tProbe outputs: [0.963, 0.7256, 0.95, 0.9657, 0.9861, 0.9912, 0.9854, 0.9896, 0.9734, 0.9862, 0.8089, 0.9707, 0.9618, 0.9693, 0.3181, 0.9857, 0.9999, 1.0, 0.9998, 0.999, 1.0, 0.9996, 1.0, 1.0, 1.0, 0.9992, 0.9737]\n",
      "Evaluating 224/316\tProbe outputs: [0.963, 0.7256, 0.95, 0.9657, 0.9861, 0.9912, 0.8519, 0.9758, 0.9756, 0.9632, 0.8114, 1.0, 1.0, 0.9997, 1.0, 1.0, 1.0, 1.0, 0.9985, 0.876, 0.9909, 0.9838, 0.9985, 0.4143, 0.9992]\n",
      "Evaluating 256/316\tProbe outputs: [0.963, 0.7256, 0.95, 0.9104, 0.9969, 0.9005, 0.7658, 0.9102, 0.9151, 0.987, 0.7915, 0.9731, 0.7789, 0.8903, 1.0, 1.0, 0.9899, 0.9732, 1.0, 0.9999, 0.9992, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Evaluating 288/316\tProbe outputs: [0.963, 0.7256, 0.95, 0.9657, 0.9979, 0.9371, 0.6444, 0.9977, 0.885, 0.9978, 0.973, 0.8505, 0.9748, 0.8668, 0.754, 0.8931, 0.9465, 0.9703, 0.9971, 0.9765, 0.9994, 1.0, 0.9883, 1.0, 0.9901, 0.9984, 0.9984, 0.9859, 0.999, 0.9997, 0.9825, 0.9996, 0.9938, 0.9776, 0.6693, 1.0]\n",
      ".Evaluating 0/316\tProbe outputs: [0.963, 0.7862, 0.8914, 0.819, 0.9103, 0.888, 0.7683, 0.818, 0.6338, 0.9432, 0.5237, 0.9527, 0.9149, 0.4643, 0.9964, 0.9948, 0.9369, 0.9645, 0.4402, 0.9787, 0.0883, 0.9726, 0.4456, 0.9439, 0.9709, 0.9829, 0.7413, 0.9899, 0.9973, 1.0, 0.9996, 0.9988, 0.8274, 0.9487, 0.9995, 0.9967, 0.9514, 0.9995, 0.9997, 0.9948]\n",
      "Evaluating 32/316\tProbe outputs: [0.963, 0.7862, 0.8914, 0.819, 0.9103, 0.888, 0.7683, 0.818, 0.6338, 0.9432, 0.5237, 0.9527, 0.9149, 0.4643, 0.9964, 0.9948, 0.9552, 0.2523, 0.9753, 0.9232, 0.8595, 0.9793, 0.7912, 0.7055, 0.5379, 0.5028, 0.8793, 0.9982, 0.9992, 0.9946, 0.8867, 0.7139, 0.9985, 0.8535, 0.9472, 0.998, 0.9925, 0.9999, 1.0, 1.0, 1.0, 0.9941, 0.9973, 0.9968, 1.0, 0.9993, 0.9999, 0.9999, 0.9995]\n",
      "Evaluating 64/316\tProbe outputs: [0.963, 0.7862, 0.8914, 0.819, 0.9103, 0.888, 0.7683, 0.818, 0.6338, 0.9432, 0.5237, 0.9527, 0.9149, 0.4643, 0.9964, 0.9948, 0.9369, 0.9988, 0.8575, 0.9471, 0.3199, 0.9899, 0.9841, 0.9987, 0.978, 0.5044, 0.8846, 0.9585, 0.9967, 1.0, 0.9987, 0.9991, 1.0, 0.9976, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Evaluating 96/316\tProbe outputs: [0.963, 0.7862, 0.8914, 0.819, 0.9103, 0.888, 0.7683, 0.818, 0.6338, 0.9432, 0.5237, 0.9527, 0.9149, 0.4643, 0.9964, 0.9948, 0.9369, 0.9811, 0.9438, 0.9977, 0.9196, 0.3738, 0.9442, 0.793, 0.6158, 0.9867, 0.9953, 0.9252, 0.9977, 0.9985, 1.0, 1.0, 1.0, 0.9995, 0.9877, 0.9992, 0.6354, 0.8917, 1.0, 1.0, 0.9998]\n",
      "Evaluating 128/316\tProbe outputs: [0.963, 0.7862, 0.8914, 0.819, 0.9103, 0.888, 0.7683, 0.818, 0.6338, 0.9432, 0.5237, 0.9527, 0.9149, 0.4643, 0.9964, 0.9948, 0.2113, 0.5348, 0.9689, 0.938, 0.9167, 0.7377, 0.9969, 0.9913, 0.9984, 0.997, 0.9996, 0.9956, 0.9957, 0.9201, 0.9997, 0.9997, 1.0, 1.0, 1.0, 0.612, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Evaluating 160/316\tProbe outputs: [0.963, 0.7862, 0.8914, 0.819, 0.9103, 0.888, 0.7683, 0.818, 0.6338, 0.9432, 0.5237, 0.9527, 0.9149, 0.4643, 0.9964, 0.9948, 0.9369, 0.9559, 0.9768, 0.598, 0.9973, 0.9693, 0.9983, 0.3428, 0.9995, 0.9984, 0.9964, 0.8998, 0.9996, 0.9981, 1.0, 1.0, 0.9939, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Evaluating 192/316\tProbe outputs: [0.963, 0.7862, 0.8914, 0.819, 0.9103, 0.888, 0.7683, 0.818, 0.6338, 0.9432, 0.5237, 0.9527, 0.9149, 0.4643, 0.9964, 0.9948, 0.9369, 0.8385, 0.9596, 0.817, 0.9708, 0.9514, 0.9899, 0.9062, 0.9461, 0.622, 0.8572, 0.2367, 0.9979, 0.9994, 1.0, 0.9996, 0.9989, 0.9934, 0.9851, 0.9994, 0.9997, 0.997, 0.9131, 0.9435]\n",
      "Evaluating 224/316\tProbe outputs: [0.963, 0.7862, 0.8914, 0.819, 0.9103, 0.888, 0.7683, 0.818, 0.6338, 0.9432, 0.5237, 0.9527, 0.9149, 0.4643, 0.9964, 0.9948, 0.9369, 0.8385, 0.9596, 0.4817, 0.9741, 0.9038, 0.9935, 0.8562, 0.9991, 1.0, 0.9999, 0.9993, 0.9997, 0.9999, 1.0, 0.9878, 1.0, 1.0, 1.0, 1.0]\n",
      "Evaluating 256/316\tProbe outputs: [0.963, 0.7862, 0.8914, 0.819, 0.9103, 0.888, 0.7683, 0.818, 0.6338, 0.9432, 0.5237, 0.9527, 0.9149, 0.4643, 0.9964, 0.9948, 0.9667, 0.9336, 0.8995, 0.4908, 0.8025, 0.9474, 0.8573, 0.4377, 0.5408, 0.8213, 0.8071, 0.9968, 1.0, 0.9903, 0.9408, 1.0, 0.9997, 0.9991, 1.0, 1.0, 1.0, 1.0, 0.9998]\n",
      "Evaluating 288/316\tProbe outputs: [0.963, 0.7862, 0.8914, 0.819, 0.9103, 0.888, 0.7683, 0.818, 0.6338, 0.9432, 0.5237, 0.9527, 0.9149, 0.4643, 0.9964, 0.9948, 0.9369, 0.9988, 0.8575, 0.9471, 0.994, 0.8822, 0.9992, 0.9938, 0.7766, 0.6447, 0.899, 0.5793, 0.6921, 0.9404, 0.894, 0.9957, 0.9674, 0.998, 1.0, 0.994, 0.9998, 0.9999, 0.9999, 0.9984, 0.9999, 0.9991, 0.9995, 0.9914, 0.9999]\n",
      ".Evaluating 0/122\tProbe outputs: [0.963, 0.7862, 0.8914, 0.925, 0.2837, 0.2463, 0.9599, 0.611, 0.9527, 0.5916, 0.3894, 0.992, 0.9931, 0.9509, 0.9221, 0.938, 0.9966, 0.9444, 0.6135, 0.9564, 0.8106, 0.6563, 0.9961, 0.9964, 0.8942, 0.3899, 0.8895, 0.5742, 0.9723, 0.9998, 1.0, 1.0, 0.4318, 0.8949, 0.7888, 0.8117, 0.0806, 0.2046, 0.9225, 0.9512]\n",
      "Evaluating 13/122\tProbe outputs: [0.963, 0.7862, 0.8914, 0.925, 0.2837, 0.2463, 0.9599, 0.611, 0.9527, 0.5916, 0.3894, 0.992, 0.9931, 0.9509, 0.9221, 0.938, 0.9966, 0.9444, 0.6135, 0.9676, 0.7035, 0.8287, 0.9913, 0.8138, 0.2646, 0.3078, 0.9227, 0.134, 0.417, 0.9039, 0.2115, 0.1458, 0.3866, 0.0, 0.0, 0.0014, 0.0, 0.0, 0.0066, 0.0009, 0.0, 0.0782]\n",
      "Evaluating 26/122\tProbe outputs: [0.963, 0.7862, 0.8914, 0.925, 0.2837, 0.2463, 0.9599, 0.611, 0.9527, 0.5916, 0.3894, 0.992, 0.9931, 0.9509, 0.9221, 0.938, 0.9966, 0.9444, 0.6135, 0.9676, 0.7035, 0.8287, 0.9913, 0.8138, 0.2646, 0.3078, 0.9227, 0.134, 0.417, 0.9039, 0.2115, 0.1458, 0.019, 0.0, 0.0002, 0.0112, 0.3111, 0.7116, 0.0007]\n",
      "Evaluating 39/122\tProbe outputs: [0.963, 0.7862, 0.8914, 0.925, 0.2837, 0.2463, 0.9599, 0.611, 0.9527, 0.5916, 0.3894, 0.992, 0.9931, 0.9509, 0.9221, 0.938, 0.9966, 0.9444, 0.6135, 0.9676, 0.7035, 0.8287, 0.9913, 0.8138, 0.2646, 0.3078, 0.9227, 0.134, 0.417, 0.9039, 0.2115, 0.1458, 0.8731, 0.0103, 0.3584, 0.1879, 0.0038, 0.0209, 0.2382, 0.003, 0.0795, 0.8071, 0.813, 0.9577, 0.0044]\n",
      "Evaluating 52/122\tProbe outputs: [0.963, 0.7862, 0.8914, 0.925, 0.2837, 0.2463, 0.9599, 0.611, 0.9527, 0.5916, 0.3894, 0.992, 0.9931, 0.9509, 0.9221, 0.938, 0.9966, 0.9444, 0.6135, 0.9564, 0.8106, 0.6563, 0.9961, 0.9964, 0.8942, 0.3899, 0.8895, 0.5742, 0.9723, 0.9998, 1.0, 0.9989, 0.9765, 0.9931, 0.8057, 0.6632, 0.6155, 0.9574, 0.3301, 0.6155, 0.3901]\n",
      "Evaluating 65/122\tProbe outputs: [0.963, 0.7862, 0.8914, 0.925, 0.2837, 0.2463, 0.9599, 0.611, 0.9527, 0.5916, 0.3894, 0.992, 0.9931, 0.9509, 0.9221, 0.938, 0.9966, 0.9444, 0.6135, 0.9676, 0.7035, 0.8287, 0.9913, 0.8138, 0.2646, 0.3078, 0.9227, 0.134, 0.417, 0.9039, 0.2115, 0.1458, 0.1955, 0.0051, 0.0002, 0.0001, 0.0001, 0.0363, 0.0138, 0.0042, 0.0008, 0.0023]\n",
      "Evaluating 78/122\tProbe outputs: [0.963, 0.7862, 0.8914, 0.925, 0.2837, 0.2463, 0.9599, 0.611, 0.9527, 0.5916, 0.3894, 0.992, 0.9931, 0.9509, 0.9221, 0.938, 0.9966, 0.9444, 0.6135, 0.9564, 0.8106, 0.6563, 0.9961, 0.9964, 0.8942, 0.3899, 0.8895, 0.5742, 0.9723, 0.9998, 1.0, 0.9989, 0.7126, 0.9987, 0.8839, 0.7194, 0.687, 0.9639, 0.2352, 0.2174, 0.7206, 0.9843, 0.8815, 0.9952]\n",
      "Evaluating 91/122\tProbe outputs: [0.963, 0.7862, 0.8914, 0.925, 0.2837, 0.2463, 0.9599, 0.611, 0.9527, 0.5916, 0.3894, 0.992, 0.9931, 0.9509, 0.9221, 0.938, 0.9966, 0.9444, 0.6135, 0.9564, 0.8106, 0.6563, 0.9961, 0.9964, 0.8942, 0.3899, 0.8895, 0.5742, 0.9723, 0.9998, 1.0, 0.9989, 0.7126, 0.4875, 0.9789, 0.6861, 0.774, 0.9502, 0.8814, 0.9742, 0.9238, 0.8189]\n",
      "Evaluating 104/122\tProbe outputs: [0.963, 0.7862, 0.8914, 0.925, 0.2837, 0.2463, 0.9599, 0.611, 0.9527, 0.5916, 0.3894, 0.992, 0.9931, 0.9509, 0.9221, 0.938, 0.9966, 0.9444, 0.6135, 0.9564, 0.8106, 0.6563, 0.9961, 0.9964, 0.8942, 0.3899, 0.8895, 0.5742, 0.9723, 0.9998, 1.0, 0.9989, 0.9765, 0.025, 0.6346, 0.9998, 0.8499, 0.9537, 0.6481, 0.9942]\n",
      "Evaluating 117/122\tProbe outputs: [0.963, 0.7862, 0.8914, 0.925, 0.2837, 0.2463, 0.9599, 0.611, 0.9527, 0.5916, 0.3894, 0.992, 0.9931, 0.9509, 0.9221, 0.938, 0.9966, 0.9444, 0.6135, 0.9676, 0.7035, 0.8287, 0.9913, 0.8138, 0.2646, 0.3078, 0.9227, 0.134, 0.417, 0.9039, 0.2115, 0.1458, 0.6929, 0.0003, 0.0002, 0.0125, 0.0, 0.0, 0.0001, 0.0091, 0.0002, 0.0905]\n",
      "."
     ]
    }
   ],
   "source": [
    "config['layer'] = layer\n",
    "rocs = evaluate_probe(probe_dataset_name, probe, config, eval_AmongUs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9997222222222222)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocs['RepEng']['auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocs['DQA']['auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amongus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
